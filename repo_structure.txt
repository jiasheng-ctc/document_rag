Repository Structure and Code:

[.]
  - chromadb_setup.py
    [Code Start]
    import chromadb
    from core.embedding import MyEmbeddingFunction
    import logging
    import shutil
    import os
    
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    # ChromaDB setup
    def setup_chromadb(path: str = "chroma_db", session_id: str = None):
        """
        Set up the ChromaDB client and create or retrieve a collection for storing document embeddings.
        Each session gets its own collection.
    
        Args:
            path (str, optional): The file path for the ChromaDB persistent storage. Default is "./chroma_db".
            session_id (str, optional): Unique identifier for the session. If provided, will create a session-specific collection.
    
        Returns:
            chromadb.api.Collection: The ChromaDB collection with embeddings ready for document storage and retrieval.
        """
        try:
            # Initialize the embedding function
            my_embed = MyEmbeddingFunction()
            
            # Verify embedding dimension by testing with a simple embedding
            test_embed = my_embed(["Test embedding initialization"])
            if test_embed and isinstance(test_embed, list) and len(test_embed) > 0:
                embedding_dim = len(test_embed[0])
                logger.info(f"Embedding function initialized with dimension: {embedding_dim}")
            else:
                logger.warning("Could not verify embedding dimension during initialization")
    
            # Initialize a persistent ChromaDB client
            chroma_client = chromadb.PersistentClient(path=path)
    
            # Create a collection name based on session_id if provided, otherwise use default
            collection_name = f"{session_id}" if session_id else "documents"
    
            # Create or retrieve the collection with custom embedding function
            collection = chroma_client.get_or_create_collection(
                name=collection_name, embedding_function=my_embed
            )
    
            logger.info(f"✅ ChromaDB setup complete. Collection '{collection_name}' is ready at: {path}")
            return collection
        except Exception as e:
            logger.error(f"Error setting up ChromaDB: {e}")
            raise
    
    # Update the cleanup_chromadb function in chromadb_setup.py
    
    def cleanup_chromadb(path: str = "./chroma_db", session_id: str = None):
        """
        Delete a session's collection or all collections.
        """
        try:
            if session_id:
                # Initialize a chromaDB client
                try:
                    chroma_client = chromadb.PersistentClient(path=path)
                    # Delete only the specific session's collection
                    collection_name = f"{session_id}"
                    try:
                        # First check if the collection exists
                        collections = chroma_client.list_collections()
                        collection_names = [collection.name for collection in collections]
                        
                        if collection_name in collection_names:
                            chroma_client.delete_collection(name=collection_name)
                            logger.info(f"Deleted collection '{collection_name}' for session: {session_id}")
                        else:
                            logger.warning(f"Collection '{collection_name}' not found for deletion")
                    except Exception as e:
                        logger.error(f"Error deleting collection {collection_name}: {e}")
                except Exception as e:
                    logger.error(f"Error connecting to ChromaDB: {e}")
            else:
                # On server restart, delete all collections by removing the DB directory
                if os.path.exists(path):
                    try:
                        # List all collections first for logging
                        try:
                            chroma_client = chromadb.PersistentClient(path=path)
                            collections = chroma_client.list_collections()
                            collection_names = [collection.name for collection in collections]
                            logger.info(f"Found collections to delete: {collection_names}")
                            
                            # Delete each collection explicitly
                            for collection in collections:
                                try:
                                    chroma_client.delete_collection(name=collection.name)
                                    logger.info(f"Deleted collection: {collection.name}")
                                except Exception as e:
                                    logger.error(f"Error deleting collection {collection.name}: {e}")
                        except Exception as e:
                            logger.error(f"Error listing collections: {e}")
                        
                        # Then remove the directory as a fallback
                        shutil.rmtree(path)
                        logger.info(f"Deleted all ChromaDB collections at {path}")
                    except Exception as e:
                        logger.error(f"Error removing ChromaDB directory: {e}")
                    # Recreate the directory
                    os.makedirs(path, exist_ok=True)
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
    
    def list_collections(path: str = "./chroma_db"):
        """List all collections in the ChromaDB - compatible with all versions"""
        try:
            chroma_client = chromadb.PersistentClient(path=path)
            collections = chroma_client.list_collections()
            
            # Extract collection names
            collection_names = []
            for collection in collections:
                try:
                    collection_names.append(collection.name)
                except AttributeError:
                    # For older versions of ChromaDB
                    if isinstance(collection, str):
                        collection_names.append(collection)
                    else:
                        try:
                            collection_names.append(str(collection))
                        except:
                            pass
            
            logger.info(f"Found collections: {collection_names}")
            return collection_names
        except Exception as e:
            logger.error(f"Error listing collections: {e}")
            return []    [Code End]
  - debug_connection.py
    [Code Start]
    import requests
    import sys
    import logging
    from chromadb_setup import setup_chromadb
    from core.embedding import ollama_embeddings
    
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    def test_ollama_connection():
        """Test connection to Ollama server"""
        try:
            logger.info("Testing connection to Ollama server...")
            response = requests.get("http://localhost:11434/api/tags")
            response.raise_for_status()
            models = response.json().get("models", [])
            
            if not models:
                logger.warning("Connected to Ollama, but no models found!")
                return False
                
            logger.info(f"Successfully connected to Ollama. Available models: {[m['name'] for m in models]}")
            return True
        except Exception as e:
            logger.error(f"Failed to connect to Ollama: {e}")
            return False
    
    def test_embedding():
        """Test the embedding functionality"""
        try:
            logger.info("Testing embedding generation...")
            embedding = ollama_embeddings("This is a test query")
            
            if isinstance(embedding, list) and len(embedding) > 0:
                logger.info(f"Successfully generated embedding with {len(embedding)} dimensions")
                return True
            else:
                logger.warning(f"Embedding generation returned unexpected result: {embedding}")
                return False
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return False
    
    def test_embedding_dimensions():
        """Test the embedding dimensions to ensure consistency"""
        try:
            logger.info("Testing embedding dimensions...")
            
            # Test with different text inputs
            texts = [
                "This is a short test",
                "This is a longer test with more words to see if length affects embedding",
                "Another test to verify embedding dimensions"
            ]
            
            dimensions = set()
            for text in texts:
                embedding = ollama_embeddings(text)
                if isinstance(embedding, list):
                    dim = len(embedding)
                    dimensions.add(dim)
                    logger.info(f"Generated embedding with {dim} dimensions")
                else:
                    logger.warning(f"Failed to generate embedding: {embedding}")
            
            if len(dimensions) == 1:
                logger.info(f"All embeddings have consistent dimension: {next(iter(dimensions))}")
                return True
            else:
                logger.error(f"Inconsistent embedding dimensions detected: {dimensions}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to test embedding dimensions: {e}")
            return False
    
    def test_chromadb():
        """Test ChromaDB setup and basic functionality"""
        try:
            logger.info("Testing ChromaDB setup...")
            collection = setup_chromadb()
            
            # Try adding a test document
            test_text = "This is a test document for ChromaDB"
            embedding = ollama_embeddings(test_text)
            
            if isinstance(embedding, list):
                collection.upsert(
                    ids=["test_document"],
                    documents=[test_text],
                    embeddings=[embedding]
                )
                
                # Try querying it back
                results = collection.query(
                    query_texts=["test"],
                    n_results=1
                )
                
                if results and len(results["documents"]) > 0:
                    logger.info("Successfully added and retrieved document from ChromaDB")
                    return True
                else:
                    logger.warning("Could not retrieve document from ChromaDB")
                    return False
            else:
                logger.warning("Could not generate embedding for ChromaDB test")
                return False
        except Exception as e:
            logger.error(f"Failed to set up or use ChromaDB: {e}")
            return False
    
    if __name__ == "__main__":
        success = True
        
        # Test Ollama connection
        if not test_ollama_connection():
            success = False
            
        # Test embedding
        if not test_embedding():
            success = False
        
        # Test embedding dimensions
        if not test_embedding_dimensions():
            success = False
        
        # Test ChromaDB
        if not test_chromadb():
            success = False
        
        if success:
            logger.info("All connection tests passed! You can now start the FastAPI application.")
            sys.exit(0)
        else:
            logger.error("One or more connection tests failed. Please check the logs above.")
            sys.exit(1)    [Code End]
  [.github]
    [workflows]
  [scripts]
    - run_fastapi.py
      [Code Start]
      from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Request
      from fastapi.middleware.cors import CORSMiddleware
      from pydantic import BaseModel
      import PyPDF2
      import base64
      import io
      import uuid
      import uvicorn
      from typing import List, Dict, Any, Optional
      from core.query import query_documents
      from core.generate import (
          generate_QUESTIONFROMDOC,
          generate_SUMMARIZATION,
          detect_task,
      )
      from core.document_utils import split_text
      from core.embedding import ollama_embeddings
      from chromadb_setup import setup_chromadb, cleanup_chromadb, list_collections
      import logging
      import os
      import atexit
      
      # Set up logging
      logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
      logger = logging.getLogger(__name__)
      
      app = FastAPI(
          title="LLaMA-based RAG System",
          description="A FastAPI based RAG system using LLaMA model and ChromaDB - Session-based PDF Only Mode with Multiple PDF Support",
          version="1.1.0"
      )
      
      # Add CORS middleware
      app.add_middleware(
          CORSMiddleware,
          allow_origins=["*"],  # In production, replace with specific origins
          allow_credentials=True,
          allow_methods=["*"],
          allow_headers=["*"],
      )
      
      # Session management - store conversations by session ID
      CONVERSATIONS = {}
      
      class Question(BaseModel):
          question: str
          pdf_contents: Optional[List[str]] = None
          session_id: Optional[str] = None
      
      class Message(BaseModel):
          role: str
          content: str
      
      class ConversationResponse(BaseModel):
          session_id: str
          conversation: List[Dict[str, str]]
      
      class DeleteSessionRequest(BaseModel):
          session_id: str
      
      class DeleteDocumentsRequest(BaseModel):
          session_id: str
          pdf_names: Optional[List[str]] = None
          force_clear: Optional[bool] = False
      
      @app.on_event("startup")
      async def startup_event():
          """Cleanup on server startup"""
          logger.info("Server starting up - cleaning up all existing collections")
          try:
              # Get existing collections for logging
              collections = list_collections()
              if collections:
                  logger.info(f"Found existing collections: {collections}")
                  
              # Use the cleanup function to delete all collections
              cleanup_chromadb()
              
              # Verify all collections were deleted
              collections_after = list_collections()
              if collections_after:
                  logger.warning(f"Some collections still exist after cleanup: {collections_after}")
              else:
                  logger.info("All collections successfully deleted on startup")
          except Exception as e:
              logger.error(f"Error during startup cleanup: {e}")
      
      
      def get_session_id(request: Request, question: Question):
          """Get or create a session ID for the conversation."""
          if question.session_id:
              return question.session_id
          
          # Create a new session ID if none provided
          return str(uuid.uuid4())
      
      def get_conversation(session_id: str) -> List[Dict[str, str]]:
          """Get the conversation for a given session ID or create a new one."""
          if session_id not in CONVERSATIONS:
              CONVERSATIONS[session_id] = []
          return CONVERSATIONS[session_id]
      
      def process_pdf(file_content):
          """Extract text from the uploaded PDF file with improved formatting."""
          text = ""
          try:
              pdf_bytes = io.BytesIO(file_content)
              pdf_reader = PyPDF2.PdfReader(pdf_bytes)
              
              # Check if PDF is encrypted
              if pdf_reader.is_encrypted:
                  try:
                      # Try with empty password
                      pdf_reader.decrypt('')
                  except:
                      raise ValueError("PDF is encrypted and could not be decrypted")
              
              for page_num in range(len(pdf_reader.pages)):
                  page = pdf_reader.pages[page_num]
                  page_text = page.extract_text()
                  
                  # Clean up the extracted text - remove excessive spaces
                  # This regex replaces multiple spaces with a single space
                  import re
                  page_text = re.sub(r'\s+', ' ', page_text)
                  
                  text += page_text + "\n"
              
              logger.info(f"Processed PDF: {len(text)} characters extracted")
              return text
          except Exception as e:
              logger.error(f"Error processing PDF: {str(e)}")
              raise ValueError(f"Error processing PDF: {str(e)}")
      
      def clean_chunk_text(text):
          import re
          
          text = re.sub(r'\s+', ' ', text)
          
          text = re.sub(r'(\$?\s*(\d)\s*(\d)\s*(\.\s*(\d)\s*(\d)))', 
                        lambda m: f"{m.group(2)}{m.group(3)}.{m.group(5)}{m.group(6)}", 
                        text)
          
          text = re.sub(r'\b[a-zA-Z]\b', '', text)
          
          replacements = {
              'S G G S T': 'SG GST',
              'G S T': 'GST',
              'N B': 'NB',
              ' l t ': ' ',
          }
          for old, new in replacements.items():
              text = text.replace(old, new)
          
          text = re.sub(r'([.,!?]){2,}', r'\1', text)
          
          text = re.sub(r'[^\w\s.,\-$()]', '', text)
          
          return ' '.join(text.split()).strip()
      
      async def task(query: str, collection, pdf_files: bool, conversation: List) -> str:
          """Determine the task and generate an appropriate response - PDF Only Mode."""
          logger.info(f"Processing query: {query}")
          category = detect_task(str(conversation), query)
          logger.info(f"Detected task category: {category}")
          
          # Check if PDF is available
          if not pdf_files:
              return "Please upload PDF documents to answer questions. This system is configured to work with documents only."
          
          # Query relevant chunks regardless of task type
          relevant_chunks = query_documents(collection, query, n_results=4)
          
          # If no relevant chunks found
          if not relevant_chunks or len(relevant_chunks) == 0:
              return "I couldn't find relevant information in the uploaded documents to answer your question. Please try asking something related to the content of the documents."
          
          # Determine if summarization or question
          if "SUMMARIZATION" in category:
              respond = generate_SUMMARIZATION(query, relevant_chunks, conversation)
          else:
              # Default to document Q&A for all other categories
              respond = generate_QUESTIONFROMDOC(query, relevant_chunks, conversation)
          
          return respond
      
      @app.get("/session-collections")
      async def get_session_collections():
          """Get all session collections for initialization"""
          collections = list_collections()
          session_ids = collections
          session_collections = collections
          
          return {
              "collections": collections,
              "session_collections": session_collections,
              "session_ids": session_ids
          }
      
      @app.get("/diagnostics")
      async def diagnostics():
          """Check system diagnostics"""
          import os
          
          # Check ChromaDB directory
          chroma_path = "./chroma_db"
          
          try:
              # Check if directory exists
              dir_exists = os.path.exists(chroma_path)
              is_dir = os.path.isdir(chroma_path) if dir_exists else False
              
              # Check directory contents
              dir_contents = os.listdir(chroma_path) if is_dir else []
              
              # Check collections
              collections = list_collections()
              
              return {
                  "directory_exists": dir_exists,
                  "is_directory": is_dir,
                  "directory_contents": dir_contents,
                  "collections": collections
              }
          except Exception as e:
              return {"error": str(e)}
      
      @app.post("/ask", response_model=ConversationResponse)
      async def ask(
          question: Question,
          background_tasks: BackgroundTasks,
          session_id: str = Depends(get_session_id)
      ):
          """API endpoint to handle the user's query and multiple PDF uploads."""
          if not question.question or not question.question.strip():
              raise HTTPException(status_code=400, detail="Please enter a valid question")
          
          # Log the session ID for debugging
          logger.info(f"Processing request for session: {session_id}")
          
          # Get or create conversation for this session
          conversation = get_conversation(session_id)
          
          # Setup session-specific ChromaDB collection
          try:
              collection = setup_chromadb(session_id=session_id)
              logger.info(f"Collection for session {session_id} is ready")
          except Exception as e:
              logger.error(f"Error setting up ChromaDB: {e}")
              raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
          
          pdf_processed_before = True
          
          try:
      
              test_query = collection.query(
                  query_texts=["test"],
                  n_results=1
              )
              if not test_query or not test_query.get("documents") or len(test_query.get("documents")[0]) == 0:
                  pdf_processed_before = False
                  logger.info(f"No documents found for session {session_id}, will process PDFs if provided")
          except Exception as e:
              pdf_processed_before = False
              logger.info(f"Error checking for existing documents: {e}")
          
          # Process PDFs if provided and not processed before
          pdf_processed = pdf_processed_before
          if question.pdf_contents and len(question.pdf_contents) > 0 and not pdf_processed_before:
              try:
                  for i, pdf_content in enumerate(question.pdf_contents):
                      try:
                          pdf_data = base64.b64decode(pdf_content)
                          doc = process_pdf(pdf_data)
                          chunks = split_text(doc)
                          
                          logger.info(f"Processing PDF #{i+1} with {len(chunks)} chunks for session {session_id}")
                          
                          # Process chunks immediately
                          for j, chunk in enumerate(chunks):
                              # Clean up the chunk text before embedding
                              cleaned_chunk = clean_chunk_text(chunk)
                              
                              # Log original vs cleaned for debugging
                              if cleaned_chunk != chunk:
                                  logger.debug(f"Cleaned chunk {j+1}:\nBefore: {chunk[:50]}...\nAfter: {cleaned_chunk[:50]}...")
                              
                              embedding = ollama_embeddings(cleaned_chunk)
                              if isinstance(embedding, list):
                                  document_id = f"session_{session_id}_pdf{i+1}_chunk{j+1}"
                                  collection.upsert(
                                      ids=[document_id],
                                      documents=[cleaned_chunk],  # Use cleaned text
                                      embeddings=[embedding]
                                  )
                          
                          pdf_processed = True
                      except Exception as e:
                          logger.error(f"Error processing PDF #{i+1}: {str(e)}")
                  
                  if pdf_processed:
                      logger.info(f"PDF processing complete for session {session_id}")
                  elif not pdf_processed_before:  # Only raise error if we haven't processed PDFs before
                      raise HTTPException(status_code=400, detail="None of the PDFs could be processed successfully")
              except Exception as e:
                  logger.error(f"Error processing PDFs: {str(e)}")
                  raise HTTPException(status_code=400, detail=f"Error processing PDFs: {str(e)}")
          
          # Generate response
          response = await task(question.question, collection, pdf_processed or pdf_processed_before, conversation)
          
          # Update conversation
          conversation.append({"role": "user", "content": question.question})
          conversation.append({"role": "assistant", "content": response})
          
          # Return the session ID with the response
          return ConversationResponse(
              session_id=session_id,
              conversation=conversation
          )
      
      @app.delete("/documents")
      async def delete_documents(request: DeleteDocumentsRequest):
          """API endpoint to delete document embeddings for a session."""
          session_id = request.session_id
          
          logger.info(f"Document deletion requested for session {session_id}")
          
          try:
              # Check if collection exists first
              collections = list_collections()
              if session_id in collections:
                  # Delete the collection
                  cleanup_chromadb(session_id=session_id)
                  
                  # Verify deletion was successful
                  collections_after = list_collections()
                  if session_id not in collections_after:
                      logger.info(f"Successfully deleted collection for session {session_id}")
                      return {"success": True, "message": f"Cleared all documents for session {session_id}"}
                  else:
                      logger.error(f"Failed to delete collection for session {session_id}")
                      return {"success": False, "error": f"Collection still exists after deletion attempt"}
              else:
                  logger.info(f"No collection found for session {session_id}")
                  return {"success": True, "message": f"No documents found for session {session_id}"}
          except Exception as e:
              logger.error(f"Error deleting document embeddings: {e}")
              return {"success": False, "error": str(e)}
      
      @app.delete("/session")
      async def delete_session(request: DeleteSessionRequest):
          """API endpoint to delete a session and its associated embeddings."""
          session_id = request.session_id
          
          # Delete conversation
          if session_id in CONVERSATIONS:
              del CONVERSATIONS[session_id]
              logger.info(f"Deleted conversation for session: {session_id}")
          
          # Clean up session's ChromaDB collection
          cleanup_chromadb(session_id=session_id)
          logger.info(f"Cleaned up embeddings for session: {session_id}")
          
          return {"success": True, "message": f"Session {session_id} deleted successfully"}
      
      @app.get("/list-collections")
      async def get_collections():
          """List all collections in ChromaDB (for debugging purposes)"""
          collections = list_collections()
          return {"collections": collections}
      
      @app.on_event("shutdown")
      def shutdown_event():
          """Cleanup on server shutdown"""
          logger.info("Server shutting down - cleaning up all collections")
          try:
              # Get existing collections for logging
              collections = list_collections()
              if collections:
                  logger.info(f"Found collections to clean up: {collections}")
                  
              # Use the cleanup function to delete all collections
              cleanup_chromadb()
              
              # No verification needed on shutdown
              logger.info("Cleanup complete on shutdown")
          except Exception as e:
              logger.error(f"Error during shutdown cleanup: {e}")
      
      if __name__ == "__main__":
          # Register cleanup function to run on exit
          atexit.register(cleanup_chromadb)
          uvicorn.run(app, host="0.0.0.0", port=8000)      [Code End]
    - run_gradio_with_fastapi.py
      [Code Start]
      import gradio as gr
      import PyPDF2
      import base64
      import requests
      import json
      import uuid
      import os
      import time
      import atexit
      
      # FastAPI server URL
      FASTAPI_URL = "http://localhost:8000"  # Change this if your FastAPI server is on a different host/port
      
      # Initialize the session ID and document tracking
      SESSION_ID = str(uuid.uuid4())
      UPLOADED_PDFS = []
      
      # Get the current directory to properly reference the logo
      current_dir = os.path.dirname(os.path.abspath(__file__))
      # Go up one level to find the root directory (assuming scripts is one level below root)
      root_dir = os.path.dirname(current_dir)
      logo_path = os.path.join(root_dir, "public", "logo.svg")
      
      print(f"Working directory: {os.getcwd()}")
      print(f"Logo path: {logo_path}")
      
      # Try to read the logo file directly
      logo_base64 = None
      if os.path.exists(logo_path):
          try:
              with open(logo_path, 'rb') as f:
                  logo_data = f.read()
                  # Convert to base64 for embedding directly in HTML
                  logo_base64 = base64.b64encode(logo_data).decode('utf-8')
                  print(f"Successfully loaded logo: {len(logo_data)} bytes")
          except Exception as e:
              print(f"Error loading logo: {e}")
      
      # Custom CSS to match the Synapxe style
      custom_css = """
      :root {
          --primary-color: #000000;
          --secondary-color: #000000;
          --background-color: #FFFFFF;
          --text-color: #000000;
          --light-gray: #F5F5F5;
          --border-color: #E0E0E0;
      }
      
      body {
          font-family: 'Arial', sans-serif;
      }
      
      .logo-container {
          padding: 20px;
          background-color: var(--background-color);
          border-bottom: 1px solid var(--border-color);
          display: flex;
          align-items: center;
          margin-bottom: 20px;
      }
      
      .logo-container img {
          height: 60px;
      }
      
      .logo-container .tagline {
          margin-left: 15px;
          color: var(--primary-color);
          font-size: 1.2em;
          font-weight: bold;
      }
      
      .chatbot-container {
          border-radius: 10px;
          box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      }
      
      
      .action-btn {
          background-color: transparent !important;
          color: #000000 !important;  /* Black text */
          border: 1px solid var(--border-color) !important;
          padding: 8px 15px !important;
          border-radius: 20px !important;
          cursor: pointer !important;
          transition: all 0.2s ease !important;
          font-weight: normal !important;
      }
      
      .action-btn:hover {
          background-color: rgba(0, 0, 0, 0.05) !important;
          border-color: #000000 !important;
      }
      
      .status-indicator {
          padding: 10px 15px;
          border-radius: 5px;
          background-color: var(--light-gray);
          font-size: 0.9em;
      }
      
      #question textarea {
          border-radius: 20px !important;
          border: 1px solid var(--border-color) !important;
          padding: 12px 15px !important;
      }
      
      #submit {
          background-color: transparent !important;
          color: #000000 !important;  /* Black text */
          border: 1px solid var(--border-color) !important;
          border-radius: 20px !important;
          transition: all 0.2s ease !important;
      }
      
      #submit:hover {
          background-color: rgba(0, 0, 0, 0.05) !important;
          border-color: #000000 !important;
      }
      
      body, p, h1, h2, h3, button, input, textarea {
          color: #000000 !important;
      }
      
      footer p {
          color: #555555 !important;  /* Slightly lighter for footer */
      }
      
      /* Button styling - works in older Gradio versions */
      button.primary {
          background-color: var(--primary-color) !important;
          color: white !important;
      }
      
      .upload-section {
          background-color: var(--light-gray);
          border-radius: 10px;
          padding: 15px;
          margin-bottom: 15px;
      }
      
      /* Logo placeholder styling */
      .logo-placeholder {
          background-color: #f0f0f0 !important;
          color: #000000 !important;
      }
      """
      
      # Functions remain the same
      def debug_collections():
          """Debug function to check existing collections."""
          try:
              response = requests.get(f"{FASTAPI_URL}/list-collections")
              if response.status_code == 200:
                  collections = response.json().get("collections", [])
                  print(f"Server collections: {collections}")
                  
                  # Check if our session exists
                  session_collection = f"{SESSION_ID}"
                  if collections and session_collection in collections:
                      print(f"✅ Current session collection '{session_collection}' exists on server")
                  else:
                      print(f"❌ Current session collection '{session_collection}' NOT FOUND on server")
                  
                  return collections
              else:
                  print(f"Error response: {response.status_code} - {response.text}")
                  return []
          except Exception as e:
              print(f"Error checking collections: {e}")
              return []
      
      def process_pdfs(files):
          """Extract text from the uploaded PDF files and convert to base64."""
          global UPLOADED_PDFS
          
          if not files:
              return None, "No documents uploaded"
          
          processed_files = []
          for file in files:
              try:
                  # For UploadButton, file is a path to the temporary file
                  with open(file.name, 'rb') as f:
                      pdf_bytes = f.read()
                  
                  # Try to get the PDF name and size
                  file_name = os.path.basename(file.name)
                  file_size = round(len(pdf_bytes) / 1024, 2)  # Size in KB
                  
                  # Save reference for later
                  processed_files.append({
                      "name": file_name,
                      "size": file_size,
                      "content": base64.b64encode(pdf_bytes).decode('utf-8')
                  })
                  
              except Exception as e:
                  print(f"Error processing PDF {file.name}: {e}")
                  return None, f"❌ Error processing {file.name}: {str(e)}"
          
          # Update global list of PDFs
          UPLOADED_PDFS = processed_files
          
          # Create status message
          if len(processed_files) == 1:
              status = f"📄 Uploaded: {processed_files[0]['name']} ({processed_files[0]['size']} KB)"
          else:
              status = f"📄 Uploaded {len(processed_files)} documents: " + ", ".join([pdf['name'] for pdf in processed_files])
          
          return files, status
      
      def handle_question_and_documents(question: str, pdf_status: str) -> tuple:
          """Handle the user's query and process the uploaded PDFs."""
          global SESSION_ID, UPLOADED_PDFS
          
          if not question.strip():
              return [{"role": "user", "content": "Please enter a valid question."}], ""
          
          # Prepare PDF content for the API call
          pdf_contents = [pdf["content"] for pdf in UPLOADED_PDFS] if UPLOADED_PDFS else []
      
          # Prepare the request to the FastAPI server
          payload = {
              "question": question,
              "pdf_contents": pdf_contents,
              "session_id": SESSION_ID
          }
          
          try:
              # Send the request to the FastAPI server
              print(f"Sending request to FastAPI with {len(pdf_contents)} PDFs for session {SESSION_ID}")
              response = requests.post(f"{FASTAPI_URL}/ask", json=payload, timeout=300)
              response.raise_for_status()
              
              # Parse the response
              result = response.json()
              
              # IMPORTANT: Always update the session ID with what the server returned
              if "session_id" in result:
                  server_session_id = result["session_id"]
                  if server_session_id != SESSION_ID:
                      print(f"⚠️ Updating session ID from {SESSION_ID} to server's {server_session_id}")
                      SESSION_ID = server_session_id
              
              # Return the conversation history and empty the input
              return result.get("conversation", []), ""
          
          except requests.exceptions.RequestException as e:
              return [
                  {
                      "role": "user", 
                      "content": f"Error communicating with backend server: {str(e)}"
                  }
              ], question
      
      def create_new_session():
          """Create a new session."""
          global SESSION_ID, UPLOADED_PDFS
          old_session_id = SESSION_ID
          SESSION_ID = str(uuid.uuid4())
          UPLOADED_PDFS = []
      
          try:
              payload = {
                  "session_id": old_session_id
              }
              
              response = requests.delete(f"{FASTAPI_URL}/session", json=payload)
              if response.status_code == 200:
                  print(f"Successfully deleted old session: {old_session_id}")
          except Exception as e:
              print(f"Error deleting old session: {e}")
          
          return (
              None, 
              "No documents uploaded",  
              [] 
          )
      
      def clear_pdfs():
          """Clear uploaded PDFs and their embeddings."""
          global UPLOADED_PDFS
          
          if not UPLOADED_PDFS:
              print("No PDFs to clear")
              return None, "No documents uploaded"
          
      
          pdf_names = [pdf["name"] for pdf in UPLOADED_PDFS]
          UPLOADED_PDFS = []
          
          # Request server to delete all embeddings for this session
          try:
              print(f"Requesting deletion of document embeddings for session {SESSION_ID}")
              
              payload = {
                  "session_id": SESSION_ID
              }
              
              response = requests.delete(f"{FASTAPI_URL}/documents", json=payload)
              
              if response.status_code == 200:
                  result = response.json()
                  print(f"Server response: {result.get('message', '')}")
              else:
                  print(f"Error response: {response.status_code} - {response.text}")
          except Exception as e:
              print(f"Error requesting deletion of document embeddings: {e}")
          
          return None, "No documents uploaded"
      
      def cleanup_on_exit():
          """Clean up on exit - delete the session."""
          try:
              payload = {
                  "session_id": SESSION_ID
              }
              
              response = requests.delete(f"{FASTAPI_URL}/session", json=payload)
              if response.status_code == 200:
                  print(f"Successfully cleaned up session on exit: {SESSION_ID}")
          except Exception as e:
              print(f"Error cleaning up on exit: {e}")
      
      # Register the cleanup function to run on exit
      atexit.register(cleanup_on_exit)
      
      # Create the Gradio interface - using components compatible with older Gradio versions
      with gr.Blocks(css=custom_css) as iface:
          if logo_base64:
              logo_html = f'<div class="logo-container"><img src="data:image/svg+xml;base64,{logo_base64}" alt="Logo" style="height:60px;"><div class="tagline">Document Intelligence</div></div>'
          else:
              logo_html = '<div class="logo-container"><div class="logo-placeholder">SYNAPXE</div><div class="tagline">Document Intelligence</div></div>'
      
          gr.HTML(logo_html)
          
          # Main content
          with gr.Row():
              with gr.Column(scale=3):
                  chatbot = gr.Chatbot(
                      label="Conversation", 
                      elem_id="chatbot",
                      height=500,
                      show_label=False,
                      type="messages"  # Use message format to avoid deprecation warning
                  )
                  
                  with gr.Row():
                      question_input = gr.Textbox(
                          label="Ask a question:",
                          elem_id="question",
                          placeholder="Type your question here...",
                          lines=1,
                          show_label=False
                      )
                      submit_button = gr.Button("Send", elem_id="submit")
              
              with gr.Column(scale=1):
                  # Use HTML for styling instead of Box component
                  gr.HTML('<div class="upload-container">')
                  gr.Markdown("### Document Upload")
                  pdf_upload = gr.File(
                      label="Upload PDF Documents",
                      file_types=[".pdf"],
                      file_count="multiple"
                  )
                  pdf_status = gr.Textbox(
                      label="Document Status",
                      placeholder="No documents uploaded",
                      interactive=False,
                      elem_id="status-indicator"
                  )
                  
                  with gr.Row():
                      clear_button = gr.Button("Clear Documents", elem_classes=["action-btn"])
                      new_session_button = gr.Button("New Session", elem_classes=["action-btn"])
                  gr.HTML('</div>')  # Close the upload container div
      
      
          # Set the action for the PDF upload
          pdf_upload.change(
              process_pdfs,
              inputs=[pdf_upload],
              outputs=[pdf_upload, pdf_status]
          )
          
          # Set the action for the clear button
          clear_button.click(
              clear_pdfs,
              inputs=[],
              outputs=[pdf_upload, pdf_status]
          )
          
          # Set the action for the new session button
          new_session_button.click(
              create_new_session,
              inputs=[],
              outputs=[pdf_upload, pdf_status, chatbot]
          )
          
          # Set the action for the submit button
          submit_button.click(
              handle_question_and_documents,
              inputs=[question_input, pdf_status],
              outputs=[chatbot, question_input]
          )
          
          # Also submit when pressing Enter in the question input
          question_input.submit(
              handle_question_and_documents,
              inputs=[question_input, pdf_status],
              outputs=[chatbot, question_input]
          )
      
      # Check collections at startup
      debug_collections()
      
      # Launch the app
      if __name__ == "__main__":
          iface.launch()      [Code End]
  [public]
  [core]
    - document_utils.py
      [Code Start]
      import os
      
      
      def load_documents_from_directory(directory_path: str):
          """
          Load text documents from a specified directory.
      
          Args:
              directory_path (str): The path to the directory containing text files.
      
          Returns:
              list: A list of dictionaries where each dictionary contains 'id' (filename)
                    and 'text' (file contents).
          """
          print(f"📂 Loading documents from directory: {directory_path}")
          documents = []
      
          for filename in os.listdir(directory_path):
              if filename.endswith(".txt"):
                  file_path = os.path.join(directory_path, filename)
                  print(f"📄 Reading file: {filename}")
      
                  with open(file_path, "r", encoding="utf-8") as file:
                      documents.append({"id": filename, "text": file.read()})
      
          print(f"✅ Successfully loaded {len(documents)} documents.")
          return documents
      
      def split_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> list:
          """
          Split a large text into smaller chunks of a specified size with overlap.
          Uses smaller chunks and more overlap for better context preservation.
      
          Args:
              text (str): The text to split into chunks.
              chunk_size (int, optional): The maximum size of each chunk. Default is 500 characters (reduced from 1000).
              chunk_overlap (int, optional): The overlap between consecutive chunks. Default is 50 characters (increased from 20).
      
          Returns:
              list: A list of text chunks.
          """
          chunks = []
          start = 0
      
          print(
              f"🔀 Splitting text into chunks of size {chunk_size} with {chunk_overlap}-character overlap."
          )
      
          # Break on sentence boundaries when possible
          import re
          sentences = re.split(r'(?<=[.!?])\s+', text)
          current_chunk = ""
          
          for sentence in sentences:
              # If adding this sentence would exceed the chunk size
              if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                  # Add the current chunk to our list
                  chunks.append(current_chunk)
                  # Start a new chunk with overlap
                  last_period = current_chunk.rfind(".")
                  if last_period != -1 and last_period > len(current_chunk) - chunk_overlap:
                      # If we can find a period in the overlap region, start from there
                      overlap_text = current_chunk[last_period+1:]
                      current_chunk = overlap_text + sentence
                  else:
                      # Otherwise take the last chunk_overlap characters
                      current_chunk = current_chunk[-chunk_overlap:] + sentence
              else:
                  # Add the sentence to the current chunk
                  if current_chunk:
                      current_chunk += " " + sentence
                  else:
                      current_chunk = sentence
          
          # Don't forget the last chunk
          if current_chunk:
              chunks.append(current_chunk)
      
          print(f"✅ Text split into {len(chunks)} chunks with improved boundary handling.")
          return chunks      [Code End]
    - llama.py
      [Code Start]
      import requests
      import json
      import logging
      import time
      from typing import List, Dict, Any, Union
      
      # Set up logging
      logging.basicConfig(level=logging.INFO)
      logger = logging.getLogger(__name__)
      
      def ollama_chat(chat_history: str, model_name: str = "phi", max_retries: int = 2, timeout: int = 120) -> str:
          """
          Call the Ollama model to generate a response for the given prompt with retry logic.
      
          Args:
              chat_history: The history of chat between user and assistant.
              model_name: The name of the Ollama model to use.
              max_retries: Maximum number of retry attempts.
              timeout: Timeout for the API call in seconds.
      
          Returns:
              str: The generated response from the model, or an error message if an issue occurs.
          """
          url = "http://localhost:11434/api/chat"  # Ollama API endpoint for chat
          
          # Ensure chat history isn't too long
          if isinstance(chat_history, list) and len(chat_history) > 10:
              chat_history = chat_history[-10:]  # Keep only the last 10 messages
          
          payload = {"model": model_name, "messages": chat_history, "stream": False}
          
          # Add some model parameters to potentially speed up response
          payload["options"] = {
              "temperature": 0.1,       # Reduced from 0.7 to 0.1 for more deterministic outputs
              "top_p": 0.5,             # Reduced from 0.9 to 0.5 to restrict token sampling
              "num_predict": 512,       # Increased to allow for complete answers
              "top_k": 10,              # Added to restrict to only the most likely tokens
              "repeat_penalty": 1.2     # Added to discourage repetition which can indicate confabulation
          }
      
          retries = 0
          while retries <= max_retries:
              try:
                  logger.info(f"Sending chat request to Ollama with model: {model_name} (attempt {retries+1}/{max_retries+1})")
                  # Send POST request
                  response = requests.post(url, json=payload, timeout=timeout)
                  response.raise_for_status()  # Raise an error for bad responses
      
                  # Parse the JSON response
                  json_response = response.json()
      
                  # Extract the generated message content
                  generated_text = json_response.get("message", {}).get(
                      "content", "I couldn't understand. Could you explain more?"
                  )
                  return generated_text
      
              except requests.exceptions.Timeout:
                  logger.warning(f"Timeout error on attempt {retries+1}/{max_retries+1}. Retrying...")
                  retries += 1
                  # If this was the last retry, give up
                  if retries > max_retries:
                      return "I'm sorry, but I'm having trouble generating a response at the moment due to system load. Please try again with a simpler question or try again later."
                  # Wait before retrying
                  time.sleep(2)
                  
              except requests.exceptions.RequestException as e:
                  logger.error(f"Error calling Ollama API: {e}")
                  retries += 1
                  if retries > max_retries:
                      return f"An error occurred while generating the response: {str(e)}"
                  time.sleep(2)
      
              except json.JSONDecodeError as e:
                  logger.error(f"JSON decoding error: {e}")
                  return f"An error occurred while processing the response: {str(e)}"
      
      def ollama_generate(prompt: str, model_name: str = "phi", timeout: int = 60) -> str:
          """
          Call the Ollama model to generate a response for the given prompt.
      
          Args:
              prompt (str): The input prompt from the user.
              model_name: The name of the Ollama model to use.
              timeout: Timeout for the API call in seconds.
      
          Returns:
              str: The generated response from the model, or an error message if an issue occurs.
          """
          url = "http://localhost:11434/api/generate"  # Ollama API endpoint for generate
          
          # Limit prompt size if it's very large
          if len(prompt) > 4000:
              prompt = prompt[:4000]
              
          payload = {
              "model": model_name, 
              "prompt": prompt, 
              "stream": False,
              "options": {
                  "temperature": 0.1,       # Reduced from 0.7 to match chat parameters
                  "top_p": 0.5,             # Reduced to match chat parameters
                  "top_k": 10,              # Added to restrict token selection
                  "num_predict": 64,        # Task detection doesn't need many tokens
                  "repeat_penalty": 1.2     # Added to discourage repetition
              }
          }
      
          try:
              logger.info(f"Sending generate request to Ollama with model: {model_name}")
              # Send POST request
              response = requests.post(url, json=payload, timeout=timeout)
              response.raise_for_status()  # Raise an error for bad responses
      
              # Parse the JSON response
              json_response = response.json()
      
              # Extract the generated message content
              generated_text = json_response.get("response", "")
              return generated_text
      
          except requests.exceptions.Timeout:
              logger.error(f"Timeout error when calling Ollama generate API")
              return "QUESTION FROM DOCUMENTS"  # Default to question answering on timeout
      
          except requests.exceptions.RequestException as e:
              logger.error(f"Error calling Ollama API: {e}")
              return "QUESTION FROM DOCUMENTS"  # Default to question answering on error
      
          except json.JSONDecodeError as e:
              logger.error(f"JSON decoding error: {e}")
              return "QUESTION FROM DOCUMENTS"  # Default to question answering on parse error      [Code End]
    - generate.py
      [Code Start]
      from core.llama import ollama_chat, ollama_generate
      import logging
      import re
      
      logger = logging.getLogger(__name__)
      
      def clean_chunk_text(text):
          """
          Clean up text chunks to improve readability for the LLM.
          This helps the model better understand the content.
          """
          # Remove excessive whitespace (including between characters)
          text = re.sub(r'\s+', ' ', text)
          
          # Fix common OCR errors with dollar amounts
          # This regex looks for patterns that might be dollar amounts with spaces
          text = re.sub(r'\$\s*(\d+)\s*\.\s*(\d+)', r'$\1.\2', text)
          
          # Fix common receipt formatting issues
          text = text.replace('S G G S T', 'SG GST')  # Common receipt header issues
          text = text.replace('G S T', 'GST')  # Tax formatting
          
          # Remove any lone characters that might be OCR errors (single character surrounded by spaces)
          text = re.sub(r'\s[bcdefghijklmnopqrstuvwxyz]\s', ' ', text, flags=re.IGNORECASE)
          
          return text.strip()
      
      def detect_task(conversation: str, query: str) -> str:
          """Detect the type of task from the conversation and query."""
          # Limit conversation size to avoid timeout
          if len(conversation) > 1000:
              conversation = conversation[-1000:]
          
          prompt = (
              "You are an AI assistant tasked with analyzing the following conversation between a user and an assistant. "
              "Your goal is to determine if the user is asking for a SUMMARIZATION of a document or asking a QUESTION FROM DOCUMENTS. "
              "Choose one of these two categories that best describes the user's request: "
              "'SUMMARIZATION' or 'QUESTION FROM DOCUMENTS'."
              "\n\nHere is the conversation:\n\n" + conversation + "\n\n"
              "User's final message: " + query + "\n\n"
              "Respond only with the category name: either 'SUMMARIZATION' or 'QUESTION FROM DOCUMENTS'."
          )
      
          category = ollama_generate(prompt)
          logger.info(f"Detected task category: {category}")
          return category
      
      def generate_SUMMARIZATION(query: str, relevant_chunks: list, history: list) -> str:
          """Generate a summary based on relevant document chunks."""
          # Clean each chunk first
          cleaned_chunks = [clean_chunk_text(chunk) for chunk in relevant_chunks]
          
          # Combine relevant chunks into a single context (limit size)
          context = "\n\n".join(cleaned_chunks)
          if len(context) > 1500:
              context = context[:1500] + "... [content truncated]"
      
          # Enhanced prompt for summarization with anti-hallucination instructions
          prompt = (
              "You are a precise document summarization assistant. The user has a document that may have extraction artifacts "
              "like extra spaces between characters, especially in numbers (for example '$ 1 0 . 9 0' should be read as '$10.90'). "
              "Your task is to summarize ONLY the information provided in the document chunks below. "
              "Do NOT include any information that is not explicitly present in the document chunks. "
              "If the document chunks do not contain information related to the user's request, state this clearly. "
              "Never make up or infer information that isn't directly stated in the documents."
              "\n\nUser's request: " + query + 
              "\n\nDocument content to summarize (ONLY use this information):\n" + context + 
              "\n\nSummary (based STRICTLY on the provided document content):"
          )
      
          # Limit history size
          history = history[-3:] if len(history) > 3 else history.copy()
          history.append({"role": "user", "content": prompt})
      
          # Generate response using the Ollama model
          response = ollama_chat(history)
          return response
      
      def generate_QUESTIONFROMDOC(query: str, relevant_chunks: list, history: list) -> str:
          """Answer a question based on document context with enhanced instructions for handling poorly formatted text."""
          # Clean each chunk
          cleaned_chunks = [clean_chunk_text(chunk) for chunk in relevant_chunks]
          
          # Combine relevant chunks into a single context (limit size)
          context = "\n\n".join(cleaned_chunks)
          if len(context) > 1500:
              context = context[:1500] + "... [content truncated]"
      
          # Enhanced prompt for document Q&A with instructions for handling PDF artifacts
          prompt = (
              "You are a precise document question-answering assistant. The user has a question about information "
              "in a PDF document. The text from this PDF may have extraction artifacts like extra spaces between characters "
              "or unusual formatting. Your task is to answer the user's question using ONLY the information "
              "provided in the document chunks below. Follow these rules: "
              
              "1. If the text contains oddly spaced characters like '$ 1 0 . 9 0', interpret this as the properly formatted value '$10.90'. "
              "2. Fix any obvious formatting issues when interpreting the content. "
              "3. If the answer is not contained in the documents, explicitly state: 'I cannot find this information in the provided documents.' "
              "4. NEVER make up information not found in these documents. "
              "5. Be concise and direct in your answer. "
              "6. You do not need to tell information can be found in which document chunks. "
              
              "\n\nQuestion: " + query + 
              "\n\nDocument content (this may contain PDF extraction artifacts like extra spaces between characters):\n" + context + 
              "\n\nAnswer (interpret any formatting issues and answer based STRICTLY on the provided content):"
          )
      
          # Limit history size
          history = history[-3:] if len(history) > 3 else history.copy()
          history.append({"role": "user", "content": prompt})
      
          # Generate response using the Ollama model
          response = ollama_chat(history)
          return response      [Code End]
    - embedding.py
      [Code Start]
      import requests
      import json
      from chromadb import Documents, EmbeddingFunction, Embeddings
      from typing import Union, List, Dict, Any
      import time
      import logging
      
      # Set up logging
      logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
      logger = logging.getLogger(__name__)
      
      # Define the default embedding dimension for LLaMA models
      # LLaMA 3's embedding dimension is 4096, but we'll check this dynamically
      DEFAULT_EMBEDDING_DIM = 4096
      
      # Update in ollama_embeddings function
      def ollama_embeddings(prompt: str, max_retries: int = 3, timeout: int = 300) -> Union[List[float], str]:
          """
          Call the Ollama model to generate embeddings for the given prompt.
      
          Args:
              prompt (str): The input prompt for embedding generation.
              max_retries (int, optional): Maximum number of retries on failure. Default is 3.
              timeout (int, optional): Timeout for the API request in seconds. Default is 300.
      
          Returns:
              Union[List[float], str]: The embedding as a list of floats, or an error message if an issue occurs.
          """
          url = "http://localhost:11434/api/embeddings"  # Ollama API endpoint for embeddings
          
          # Use the same model as in the llama.py file for consistency
          model_name = "phi"  # This should match the model in llama.py
          
          payload = {"model": model_name, "prompt": prompt, "stream": False}
          
          retries = 0
          while retries < max_retries:
              try:
                  # Send POST request with timeout
                  response = requests.post(url, json=payload, timeout=timeout)
                  response.raise_for_status()  # Raise an error for bad responses
      
                  # Parse the JSON response
                  json_response = response.json()
      
                  # Extract the embeddings
                  embeddings = json_response.get("embedding", [])
                  if embeddings and isinstance(embeddings, list):
                      logger.info(f"Successfully generated embedding with {len(embeddings)} dimensions using {model_name}")
                  return embeddings
      
              except requests.exceptions.RequestException as e:
                  logger.error(f"Error calling Ollama API (attempt {retries+1}/{max_retries}): {e}")
                  retries += 1
                  if retries < max_retries:
                      # Exponential backoff
                      time.sleep(2 ** retries)
                      continue
                  return f"An error occurred while generating the embeddings: {str(e)}"
      
              except json.JSONDecodeError as e:
                  logger.error(f"JSON decoding error: {e}")
                  return f"An error occurred while processing the embeddings response: {str(e)}"
      
      class MyEmbeddingFunction(EmbeddingFunction):
          """
          A custom embedding function that uses the Ollama model to generate embeddings for input documents.
          """
          
          def __init__(self, model_name: str = "phi", timeout: int = 60):
              """
              Initialize the embedding function with a model name and timeout.
              
              Args:
                  model_name (str): Name of the Ollama model to use
                  timeout (int): Timeout for API calls in seconds
              """
              self.model_name = model_name
              self.timeout = timeout
              self.url = "http://localhost:11434/api/embeddings"
              self.embedding_dim = None  # Will be set dynamically on first call
              
              # Try to determine embedding dimension at initialization
              self._initialize_embedding_dim()
          
          def _initialize_embedding_dim(self):
              """Get the embedding dimension by generating a test embedding"""
              try:
                  test_embedding = ollama_embeddings("Test embedding dimension")
                  if isinstance(test_embedding, list):
                      self.embedding_dim = len(test_embedding)
                      logger.info(f"Detected LLaMA embedding dimension: {self.embedding_dim}")
                  else:
                      logger.warning("Could not determine embedding dimension, using default")
                      self.embedding_dim = DEFAULT_EMBEDDING_DIM
              except Exception as e:
                  logger.warning(f"Error detecting embedding dimension: {e}. Using default: {DEFAULT_EMBEDDING_DIM}")
                  self.embedding_dim = DEFAULT_EMBEDDING_DIM
      
          def __call__(self, input: Documents) -> Embeddings:
              """
              Generate embeddings for a list of input documents by calling the Ollama API.
      
              Args:
                  input (Documents): A list of documents (strings) for which to generate embeddings.
      
              Returns:
                  Embeddings: A list of embedding vectors corresponding to the input documents.
              """
              # Ensure input is a list
              if not isinstance(input, list):
                  input = [input]
              
              all_embeddings = []
              
              # Process each document separately to get individual embeddings
              for document in input:
                  if not document or not isinstance(document, str):
                      # Handle empty or non-string documents with a zero vector
                      logger.warning("Empty or non-string document received. Using zero vector.")
                      all_embeddings.append([0.0] * self.embedding_dim)
                      continue
                      
                  payload = {
                      "model": self.model_name,
                      "prompt": document,
                      "stream": False
                  }
                  
                  try:
                      # Send POST request to Ollama API with timeout
                      response = requests.post(self.url, json=payload, timeout=self.timeout)
                      response.raise_for_status()
                      
                      # Parse the JSON response to get embeddings
                      json_response = response.json()
                      embedding = json_response.get("embedding", [])
                      
                      # Verify embedding is valid
                      if not embedding or not isinstance(embedding, list):
                          logger.error(f"Invalid embedding received: {embedding}")
                          all_embeddings.append([0.0] * self.embedding_dim)
                      else:
                          # If this is our first successful embedding, update the dimension
                          if self.embedding_dim is None or self.embedding_dim != len(embedding):
                              self.embedding_dim = len(embedding)
                              logger.info(f"Updated embedding dimension to {self.embedding_dim}")
                              
                          all_embeddings.append(embedding)
                      
                  except Exception as e:
                      logger.error(f"Error generating embedding: {e}")
                      # Return a zero vector as a fallback
                      all_embeddings.append([0.0] * self.embedding_dim)
              
              return all_embeddings      [Code End]
    - query.py
      [Code Start]
      import logging
      
      logger = logging.getLogger(__name__)
      
      def query_documents(collection, question: str, n_results: int = 4) -> list:
          """
          Query the ChromaDB collection for documents relevant to the given question.
      
          Args:
              collection: The ChromaDB collection to query
              question (str): The input question or query text.
              n_results (int, optional): The number of relevant documents to retrieve. Default is 4.
      
          Returns:
              list: A list of relevant document chunks.
          """
          logger.info(f"Querying for relevant documents to the question: '{question}' (Top {n_results} results)")
      
          try:
              # Query the collection
              results = collection.query(
                  query_texts=question, 
                  n_results=n_results,
                  include=["documents", "distances", "metadatas"]
              )
              
              # Extract relevant document chunks with their IDs and distances
              relevant_chunks = []
              
              if results and "documents" in results and len(results["documents"]) > 0:
                  # Flatten the list of documents
                  for i, docs in enumerate(results["documents"]):
                      for j, doc in enumerate(docs):
                          # Get the document ID and distance if available
                          doc_id = results.get("ids", [[]])[i][j] if "ids" in results else f"doc_{i}_{j}"
                          distance = results.get("distances", [[]])[i][j] if "distances" in results else None
                          
                          # Add the document to the list of relevant chunks
                          relevant_chunks.append(doc)
                          
                          # Log document details
                          doc_preview = doc[:50] + "..." if len(doc) > 50 else doc
                          if distance:
                              logger.info(f"Document {doc_id} (relevance: {1-distance:.4f}): {doc_preview}")
                          else:
                              logger.info(f"Document {doc_id}: {doc_preview}")
              
              logger.info(f"Found {len(relevant_chunks)} relevant document chunks")
              return relevant_chunks
              
          except Exception as e:
              logger.error(f"Error querying documents: {e}")
              return []      [Code End]
    - web_scrape.py
      [Code Start]
      import requests
      import time
      from bs4 import BeautifulSoup
      from typing import List, Dict, Any, Optional
      import logging
      
      # Configure logging
      logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
      logger = logging.getLogger(__name__)
      
      # User agent to mimic a browser
      HEADERS = {
          "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
      }
      
      def search_duckduckgo(query: str, num_results: int = 2) -> List[str]:
          """
          Perform a search using DuckDuckGo and extract URLs from the results.
          
          Args:
              query (str): The search query
              num_results (int): Number of results to return
              
          Returns:
              List[str]: List of URLs from search results
          """
          # DuckDuckGo search URL
          search_url = f"https://html.duckduckgo.com/html/?q={query}"
          
          try:
              logger.info(f"Searching for: {query}")
              response = requests.get(search_url, headers=HEADERS, timeout=10)
              response.raise_for_status()
              
              soup = BeautifulSoup(response.text, 'html.parser')
              results = soup.find_all('a', class_='result__url')
              
              urls = []
              for result in results[:num_results]:
                  href = result.get('href')
                  if href and href.startswith('http'):
                      urls.append(href)
                  elif href:
                      # Some URLs might be relative
                      urls.append(f"https:{href}" if href.startswith('//') else f"https://{href}")
              
              logger.info(f"Found {len(urls)} URLs")
              return urls
              
          except Exception as e:
              logger.error(f"Error searching DuckDuckGo: {e}")
              return []
      
      def scrape_webpage(url: str, timeout: int = 10, max_length: int = 1000) -> str:
          """
          Scrape content from a webpage with content length limit.
          
          Args:
              url (str): The URL to scrape
              timeout (int): Request timeout in seconds
              max_length (int): Maximum length of content to return
              
          Returns:
              str: The extracted text content
          """
          try:
              logger.info(f"Scraping URL: {url}")
              response = requests.get(url, headers=HEADERS, timeout=timeout)
              response.raise_for_status()
              
              soup = BeautifulSoup(response.text, 'html.parser')
              
              # Remove script and style elements
              for script in soup(["script", "style"]):
                  script.extract()
                  
              # Extract text content
              text = soup.get_text(separator='\n')
              
              # Clean up text: remove extra newlines and spaces
              lines = (line.strip() for line in text.splitlines())
              chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
              text = '\n'.join(chunk for chunk in chunks if chunk)
              
              # Return a reasonably sized excerpt
              return text[:max_length]
              
          except Exception as e:
              logger.error(f"Error scraping {url}: {e}")
              return f"[Error scraping {url}: {str(e)}]"
      
      def scrape_web(query: str, max_results: int = 2, max_content_per_site: int = 1000) -> str:
          """
          Perform a web search and scrape the content from the top search results.
      
          Args:
              query (str): The search query to use for web scraping.
              max_results (int): Maximum number of results to scrape
              max_content_per_site (int): Maximum content length per site
      
          Returns:
              str: The combined text content of the scraped search results.
          """
          logger.info(f"Starting web search for: '{query}'")
          
          # Limit query length
          if len(query) > 100:
              query = query[:100]
          
          # Search for relevant URLs
          urls = search_duckduckgo(query, num_results=max_results)
          
          if not urls:
              return "No relevant information found on the web for this query."
          
          results = []
          
          # Scrape each URL and append the result to the output
          for url in urls:
              result = scrape_webpage(url, max_length=max_content_per_site)
              if result:
                  results.append(f"Source: {url}\n\n{result}")
              
              # Be polite with rate limiting
              time.sleep(1)
          
          logger.info(f"Completed scraping {len(results)} URLs")
          
          # Combine results with source attribution
          if results:
              combined = "\n\n---\n\n".join(results)
              # Limit total combined length
              if len(combined) > 2000:
                  combined = combined[:2000] + "... [content truncated]"
              return combined
          else:
              return "Unable to retrieve relevant information from the web for this query."      [Code End]
  [.git]
    [branches]
    [objects]
      [pack]
      [info]
    [info]
    [hooks]
    [refs]
      [heads]
      [remotes]
        [origin]
      [tags]
    [logs]
      [refs]
        [heads]
        [remotes]
          [origin]
  [chroma_db]
    [77c4e579-a024-472c-a2f0-ceae13dff6a8]
